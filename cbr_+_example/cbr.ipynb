{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys, os \n",
    "import argparse\n",
    "from itertools import combinations\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File of PDB model 2uv8 is loaded for processing, contain 6 chains: ['A', 'B', 'C', 'G', 'H', 'I']\n"
     ]
    }
   ],
   "source": [
    "# Part 0: Read file and extract basic information: (1)pdb_id, (2)chain numbers, (3)chain IDs, (4)data for each chain\n",
    "\n",
    "# Populate paths_to_process_list\n",
    "def populate_paths_to_list(path):\n",
    "    paths_to_process_list = []\n",
    "\n",
    "    if os.path.isfile(path) and path.lower().endswith('.pdb'):    # Check if the provided path is a .pdb file\n",
    "        paths_to_process_list.append(path)\n",
    "        return paths_to_process_list\n",
    "    elif os.path.isdir(path):                              # Check if the provided path is indeed a directory\n",
    "        for filename in os.listdir(path):                  # Iterate over files in the directory\n",
    "            filepath = os.path.join(path, filename)        # Construct the full file path\n",
    "            if filename.lower().endswith('.pdb'):\n",
    "                paths_to_process_list.append(filepath)\n",
    "        return paths_to_process_list\n",
    "    \n",
    "    if not paths_to_process_list:\n",
    "        print(f'No valid PDB files found in path: {path}. The program is quit, please try again.')\n",
    "        return\n",
    "\n",
    "# Extract the model id from the .pdb file\n",
    "def extract_pdb_id(filepath):\n",
    "    id, _ = os.path.splitext(os.path.basename(filepath)) # Get the basename form the file path, split the name adn the extension, just get the name\n",
    "    return id\n",
    "\n",
    "# Extract the chains id and information lines of chains from the .pdb file\n",
    "def read_file_and_extract_chain_id(pdb_file):\n",
    "    with open(pdb_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    chain_set = set()\n",
    "    for line in lines:\n",
    "        if line.startswith('ATOM'):\n",
    "            chain_id = line[21]\n",
    "            chain_set.add(chain_id)\n",
    "    # print(f'The type of chains: {type(chains)}')\n",
    "    # print(f'The type of sorted chains: {type(sorted (chains))}')\n",
    "    return chain_set, lines\n",
    "\n",
    "path = '/Users/luna/Documents/RP1/2uv8.pdb'\n",
    "paths_to_process_list = populate_paths_to_list(path)\n",
    "for pathway in paths_to_process_list:\n",
    "    pdb_id = extract_pdb_id(pathway)\n",
    "    chain_set, lines = read_file_and_extract_chain_id(pathway)\n",
    "    print(f'File of PDB model {pdb_id} is loaded for processing, contain {len(chain_set)} chains: {sorted(chain_set)}')\n",
    "    # print(f'File type: pdb_id: {type(pdb_id)} pdb_chains: {type(chain_set)} pdb_lines: {type(lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written individual-chain-file: 2uv8_A.pdb\n",
      "Written individual-chain-file: 2uv8_G.pdb\n",
      "['2uv8_A.pdb', '2uv8_G.pdb']\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Generate all individual and combined files for each PDB model\n",
    "\n",
    "# Write info of each chain into individual files\n",
    "def write_individual_files(pdb_id, chain_set, lines):\n",
    "    pdb_filenames = []\n",
    "    for chain in chain_set:\n",
    "        pdb_filename = f'{pdb_id}_{chain}.pdb'\n",
    "        with open(pdb_filename, 'w') as file:\n",
    "            for line in lines:\n",
    "                if line.startswith('ATOM') and line[21] == chain:\n",
    "                    file.write(line)\n",
    "        print(f'Written individual-chain-file: {pdb_filename}')\n",
    "        pdb_filenames.append(pdb_filename)\n",
    "    return pdb_filenames\n",
    "\n",
    "chain_set = ['A', 'G']\n",
    "individual_pdb_filename_list = write_individual_files(pdb_id, chain_set, lines)\n",
    "print(individual_pdb_filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a part of, or all chains, write info of every pair of chains into combined files\n",
    "def write_part_or_all_combined_files(pdb_id, chain_set, lines):\n",
    "    pdb_filename_list = []\n",
    "    # Get all unique combinations of two chains\n",
    "    chain_combinations = combinations(sorted(chain_set), 2)  \n",
    "    for chain_pair in chain_combinations:\n",
    "        # Skip if the chains are the same or if the reverse pair has already been processed\n",
    "        if chain_pair[0] == chain_pair[1] or (chain_pair[1], chain_pair[0]) in pdb_filename_list:\n",
    "            continue\n",
    "        pdb_filename = f'{pdb_id}_{\"\".join(chain_pair)}.pdb'\n",
    "        with open(pdb_filename, 'w') as file:\n",
    "            for line in lines:\n",
    "                if line.startswith('ATOM') and line[21] in chain_pair:\n",
    "                    file.write(line)\n",
    "        print(f'Written combined-chain-file: {pdb_filename}')\n",
    "        pdb_filename_list.append(pdb_filename)\n",
    "    return pdb_filename_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written combined-chains-file: 2uv8_AG.pdb\n",
      "['2uv8_AG.pdb']\n"
     ]
    }
   ],
   "source": [
    "# For two designated chains, write info of this pair into a combined file\n",
    "def write_designated_combined_files(pdb_id, chain_set, lines):\n",
    "    pdb_filename_list = []\n",
    "    if len(chain_set) == 2 and chain_set[0] != chain_set[1]:\n",
    "        pdb_filename = f'{pdb_id}_{chain_set[0]}{chain_set[1]}.pdb'\n",
    "        with open(pdb_filename, 'w') as file:\n",
    "            for line in lines:\n",
    "                if line.startswith('ATOM') and (line[21] == chain_set[0] or line[21] == chain_set[1]):\n",
    "                    file.write(line)\n",
    "        print(f\"Written combined-chains-file: {pdb_filename}\")\n",
    "        pdb_filename_list.append(pdb_filename)\n",
    "    else:\n",
    "        print(\"Error: Please specify two different chains.\")\n",
    "    return pdb_filename_list\n",
    "\n",
    "chain_set = ['A', 'G']\n",
    "combined_pdb_filename_list = write_designated_combined_files(pdb_id, chain_set, lines)\n",
    "print(combined_pdb_filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The individual .pdb files for [naccess] to process: ['2uv8_A.pdb', '2uv8_G.pdb']\n",
      "naccess: using vdw.radii in local directory\n",
      "naccess: using STD FILE in local directory\n",
      "\"naccess\" successfully processed 2uv8_A.pdb\n",
      "\n",
      "naccess: using vdw.radii in local directory\n",
      "naccess: using STD FILE in local directory\n",
      "\"naccess\" successfully processed 2uv8_G.pdb\n",
      "\n",
      "The individual .rsa files generated: ['2uv8_A.rsa', '2uv8_G.rsa']\n",
      "\n",
      "The combined .pdb files for [naccess] to process: ['2uv8_AG.pdb']\n",
      "naccess: using vdw.radii in local directory\n",
      "naccess: using STD FILE in local directory\n",
      "\"naccess\" successfully processed 2uv8_AG.pdb\n",
      "\n",
      "The combined .rsa files generated: ['2uv8_AG.rsa']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "\n",
    "# 2.1 Perform naccess on individual-chain-files and combined-chains-files\n",
    "def perform_naccess_on_files(pdb_filename):\n",
    "    rsa_filename_list = []\n",
    "    basename = pdb_filename.split('.')[0]\n",
    "    rsa_filename = f'{basename}.rsa'\n",
    "    try:\n",
    "        subprocess.run([\"./naccess\", pdb_filename], check=True)\n",
    "        rsa_filename_list.append(rsa_filename)\n",
    "        print(f'\"naccess\" successfully processed {pdb_filename}\\n')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f'Error running \"naccess\" on {pdb_filename}: {e}')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'The \"naccess\" program was not found: {e}')\n",
    "    return rsa_filename_list\n",
    "\n",
    "individual_rsa_filename_list = []\n",
    "combined_rsa_filename_list = []\n",
    "# For individual chain\n",
    "if individual_pdb_filename_list:  # Check if the list is not empty\n",
    "    print(f'The individual .pdb files for [naccess] to process: {individual_pdb_filename_list}')\n",
    "    for individual_pdb_filename in individual_pdb_filename_list:\n",
    "        individual_rsa_filename_list.extend(perform_naccess_on_files(individual_pdb_filename))\n",
    "    print(f'The individual .rsa files generated: {individual_rsa_filename_list}\\n')\n",
    "# For combined chains\n",
    "if combined_pdb_filename_list:  # Check if the list is not empty\n",
    "    print(f'The combined .pdb files for [naccess] to process: {combined_pdb_filename_list}')\n",
    "    for combined_pdb_filename in combined_pdb_filename_list:\n",
    "        combined_rsa_filename_list.extend(perform_naccess_on_files(combined_pdb_filename))\n",
    "    print(f'The combined .rsa files generated: {combined_rsa_filename_list}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame 2uv8_A is created, shape: (1613, 14)\n",
      "       0    1  2     3       4     5       6     7      8      9      10  \\\n",
      "0     RES  MET  A     1  157.83  81.3   86.87  55.5  70.96  189.2  89.98   \n",
      "1     RES  LYS  A     2  114.48  57.0  108.26  66.3   6.22   16.6  61.32   \n",
      "2     RES  PRO  A     3   97.77  71.8   92.91  77.5   4.86   29.9  94.77   \n",
      "3     RES  GLU  A     4  104.97  60.9   99.83  74.1   5.14   13.7  69.89   \n",
      "4     RES  VAL  A     5   59.26  39.1   59.10  51.7   0.16    0.4  59.10   \n",
      "...   ...  ... ..   ...     ...   ...     ...   ...    ...    ...    ...   \n",
      "1608  RES  SER  A  1743   24.51  21.0   22.95  29.4   1.57    4.1  22.50   \n",
      "1609  RES  TYR  A  1744   88.21  41.5   85.62  48.3   2.60    7.3  60.77   \n",
      "1610  RES  ILE  A  1745  108.70  62.1   84.62  61.3  24.09   64.8  84.62   \n",
      "1611  RES  ASN  A  1746  120.70  83.9   91.59  86.2  29.11   77.2  39.74   \n",
      "1612  RES  ALA  A  1747   84.00  77.8   27.16  39.1  56.85  147.5  45.74   \n",
      "\n",
      "         11     12     13  \n",
      "0      57.0  67.85  186.8  \n",
      "1      52.6  53.16   63.1  \n",
      "2      78.4   3.00   19.7  \n",
      "3     115.9  35.09   31.3  \n",
      "4      51.2   0.16    0.5  \n",
      "...     ...    ...    ...  \n",
      "1608   46.3   2.02    3.0  \n",
      "1609   44.5  27.44   36.0  \n",
      "1610   60.8  24.09   66.9  \n",
      "1611   86.0  80.96   82.8  \n",
      "1612   64.1  38.26  104.6  \n",
      "\n",
      "[1613 rows x 14 columns]\n",
      "The DataFrame 2uv8_G is created, shape: (2033, 14)\n",
      "       0    1  2     3       4      5       6      7      8      9       10  \\\n",
      "0     RES  SER  G     5  120.90  103.8   77.93   99.8  42.97  111.9   42.42   \n",
      "1     RES  THR  G     6   91.53   65.7   67.17   66.0  24.36   64.8   66.15   \n",
      "2     RES  ARG  G     7   73.16   30.6   72.70   36.1   0.46    1.2   38.81   \n",
      "3     RES  PRO  G     8   92.13   67.7   80.91   67.5  11.22   69.2   82.11   \n",
      "4     RES  LEU  G     9    8.52    4.8    8.43    6.0   0.09    0.2    8.43   \n",
      "...   ...  ... ..   ...     ...    ...     ...    ...    ...    ...     ...   \n",
      "2028  RES  GLU  G  2046  126.61   73.5  103.84   77.1  22.78   60.7   74.85   \n",
      "2029  RES  LYS  G  2047  127.10   63.3  114.53   70.1  12.57   33.5   76.64   \n",
      "2030  RES  TYR  G  2048  168.61   79.2  145.16   81.8  23.45   66.3  106.72   \n",
      "2031  RES  GLU  G  2049  140.38   81.5  121.61   90.3  18.76   50.0   63.93   \n",
      "2032  RES  GLN  G  2050  207.24  116.1  143.43  101.7  63.82  170.1   88.06   \n",
      "\n",
      "         11      12     13  \n",
      "0      87.4   78.49  115.5  \n",
      "1      87.4   25.38   39.9  \n",
      "2      49.9   34.36   21.3  \n",
      "3      67.9   10.02   66.0  \n",
      "4       5.9    0.09    0.3  \n",
      "...     ...     ...    ...  \n",
      "2028  124.2   51.76   46.2  \n",
      "2029   65.7   50.46   59.9  \n",
      "2030   78.2   61.90   81.2  \n",
      "2031  106.0   76.44   68.3  \n",
      "2032  168.6  119.18   94.4  \n",
      "\n",
      "[2033 rows x 14 columns]\n",
      "The DataFrame 2uv8_AG is created, shape: (3646, 14)\n",
      "       0    1  2     3       4     5       6     7      8      9      10  \\\n",
      "0     RES  MET  A     1   79.74  41.1   21.78  13.9  57.95  154.5  24.89   \n",
      "1     RES  LYS  A     2   59.19  29.5   56.42  34.6   2.77    7.4  21.17   \n",
      "2     RES  PRO  A     3   96.33  70.8   92.91  77.5   3.42   21.1  94.77   \n",
      "3     RES  GLU  A     4  103.37  60.0   99.83  74.1   3.54    9.4  69.89   \n",
      "4     RES  VAL  A     5   25.80  17.0   25.63  22.4   0.16    0.4  25.63   \n",
      "...   ...  ... ..   ...     ...   ...     ...   ...    ...    ...    ...   \n",
      "3641  RES  GLU  G  2046  126.61  73.5  103.84  77.1  22.78   60.7  74.85   \n",
      "3642  RES  LYS  G  2047   85.40  42.5   80.39  49.2   5.01   13.4  43.31   \n",
      "3643  RES  TYR  G  2048   99.30  46.7   97.89  55.2   1.41    4.0  61.06   \n",
      "3644  RES  GLU  G  2049  140.24  81.4  121.47  90.2  18.76   50.0  63.79   \n",
      "3645  RES  GLN  G  2050  140.82  78.9   79.62  56.5  61.20  163.2  44.28   \n",
      "\n",
      "         11     12     13  \n",
      "0      15.8  54.85  151.0  \n",
      "1      18.2  38.02   45.1  \n",
      "2      78.4   1.56   10.3  \n",
      "3     115.9  33.48   29.9  \n",
      "4      22.2   0.16    0.5  \n",
      "...     ...    ...    ...  \n",
      "3641  124.2  51.76   46.2  \n",
      "3642   37.2  42.09   50.0  \n",
      "3643   44.7  38.23   50.1  \n",
      "3644  105.8  76.44   68.3  \n",
      "3645   84.8  96.54   76.5  \n",
      "\n",
      "[3646 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Extract data from .rsa file and return as a pandas DataFrame\n",
    "def extract_rsa_data_into_df(rsa_filename):\n",
    "    # Define a regular expression pattern to match the lines starting with 'RES'\n",
    "    # and capture the different parts of the line.\n",
    "    line_pattern = re.compile(r'^RES\\s+(\\w{3})\\s+([A-Za-z])\\s*(\\d+)\\s+(-?[\\d\\.]+)\\s+(-?[\\d\\.]+)\\s+(-?[\\d\\.]+)\\s+(-?[\\d\\.]+)\\s+(-?[\\d\\.]+)\\s+(-?[\\d\\.]+)\\s+(-?[\\d\\.]+)\\s+(-?[\\d\\.]+)\\s+(-?[\\d\\.]+)\\s+(-?[\\d\\.]+)')\n",
    "    \n",
    "    with open(rsa_filename, 'r') as file:\n",
    "        data = []\n",
    "        for line in file:\n",
    "            match = line_pattern.match(line)\n",
    "            if match:\n",
    "                # If a match is found, extract all the captured groups\n",
    "                row = match.groups()\n",
    "                # Prepend 'RES' to the row tuple\n",
    "                new_row = ('RES',) + row\n",
    "                data.append(new_row)\n",
    "    \n",
    "    # Create a DataFrame with the correct column names\n",
    "    df = pd.DataFrame(data, columns=None)\n",
    "    \n",
    "    # Extract the base name of the file without the extension\n",
    "    df_name = rsa_filename.split('.')[0]\n",
    "    # print(f'The DataFrame {df_name} is created, shape: {df.shape}')\n",
    "    # print(f'{df}')\n",
    "    return df, df_name\n",
    "\n",
    "\n",
    "df_individual_dic = {}\n",
    "df_combined_dic = {}\n",
    "# For individual chain\n",
    "for individual_rsa_file in individual_rsa_filename_list:\n",
    "    df_individual, df_individual_name = extract_rsa_data_into_df(individual_rsa_file)\n",
    "    df_individual_dic[df_individual_name] = df_individual\n",
    "    print(f'The DataFrame {df_individual_name} is created, shape: {df_individual.shape}')\n",
    "    print(df_individual)\n",
    "# For combined chains\n",
    "for combined_rsa_file in combined_rsa_filename_list:\n",
    "    df_combined, df_combined_name = extract_rsa_data_into_df(combined_rsa_file)\n",
    "    df_combined_dic[df_combined_name] = df_combined\n",
    "    print(f'The DataFrame {df_combined_name} is created, shape: {df_combined.shape}')\n",
    "    print(df_combined)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined df: 2uv8_AG\n",
      "df_chain1: 2uv8_A\n",
      "df_chain2: 2uv8_G\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Match names\n",
    "def find_matching_individual_dfs_to_combined_df(df_combined_name):\n",
    "    \"\"\"\n",
    "    Given the name of a combined DataFrame, extract and return the names of the \n",
    "    individual DataFrames that correspond to the chains in the combined DataFrame.\n",
    "    \"\"\"\n",
    "    # Construct the names of the corresponding individual DataFrames\n",
    "    pdb_id, chain_set = df_combined_name.split('_')\n",
    "    chain1_id, chain2_id = chain_set[0], chain_set[1]\n",
    "    \n",
    "    df_individual_1_name = f\"{pdb_id}_{chain1_id}\"\n",
    "    df_individual_2_name = f\"{pdb_id}_{chain2_id}\"\n",
    "    \n",
    "    return df_individual_1_name, df_individual_2_name\n",
    "\n",
    "for df_combined_name, df_combined in df_combined_dic.items():\n",
    "    df_chain1_name, df_chain2_name = find_matching_individual_dfs_to_combined_df(df_combined_name)\n",
    "    # Fetch the corresponding individual DataFrames from dic_df_individual\n",
    "    df_chain1 = df_individual_dic.get(df_chain1_name)\n",
    "    df_chain2 = df_individual_dic.get(df_chain2_name)\n",
    "    print(f'Combined df: {df_combined_name}')\n",
    "    print(f'df_chain1: {df_chain1_name}')\n",
    "    print(f'df_chain2: {df_chain2_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort decimal places\n",
    "def format_decimal_places(df):\n",
    "    # Identify ABS and REL columns by their position (assuming ABS and REL alternate after the first 2 columns)\n",
    "    abs_columns = df.columns[4::2]  # Starting from the 5rd column, every second column is 'ABS'\n",
    "    rel_columns = df.columns[5::2]  # Starting from the 6th column, every second column is 'REL'\n",
    "    # Format ABS columns with 2 decimal places and REL columns with 1 decimal place\n",
    "    for col in abs_columns:\n",
    "        df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.loc[:, col] = df[col].apply(lambda x: f'{x:.2f}' if pd.notnull(x) else x)\n",
    "    for col in rel_columns:\n",
    "        df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.loc[:, col] = df[col].apply(lambda x: f'{x:.1f}' if pd.notnull(x) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Written subtraction file 2uv8_A-AG.csv, aa: 1613\n",
      "Written subtraction file 2uv8_G-AG.csv, aa: 2033\n",
      "2uv8_A-AG.csv: \n",
      "       0    1  2     3      4     5      6     7      8     9      10    11  \\\n",
      "0     RES  MET  A     1  78.09  40.2  65.09  41.6  13.01  34.7  65.09  41.2   \n",
      "1     RES  LYS  A     2  55.29  27.5  51.84  31.7   3.45   9.2  40.15  34.4   \n",
      "2     RES  PRO  A     3   1.44   1.0   0.00   0.0   1.44   8.8   0.00   0.0   \n",
      "3     RES  GLU  A     4   1.60   0.9   0.00   0.0   1.60   4.3   0.00   0.0   \n",
      "4     RES  VAL  A     5  33.46  22.1  33.47  29.3   0.00   0.0  33.47  29.0   \n",
      "...   ...  ... ..   ...    ...   ...    ...   ...    ...   ...    ...   ...   \n",
      "1608  RES  SER  A  1743   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "1609  RES  TYR  A  1744   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "1610  RES  ILE  A  1745   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "1611  RES  ASN  A  1746   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "1612  RES  ALA  A  1747   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "\n",
      "         12    13  \n",
      "0     13.00  35.8  \n",
      "1     15.14  18.0  \n",
      "2      1.44   9.4  \n",
      "3      1.61   1.4  \n",
      "4      0.00   0.0  \n",
      "...     ...   ...  \n",
      "1608   0.00   0.0  \n",
      "1609   0.00   0.0  \n",
      "1610   0.00   0.0  \n",
      "1611   0.00   0.0  \n",
      "1612   0.00   0.0  \n",
      "\n",
      "[1613 rows x 14 columns]\n",
      "2uv8_G-AG.csv: \n",
      "       0    1  2     3      4     5      6     7      8     9      10    11  \\\n",
      "0     RES  SER  G     5   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "1     RES  THR  G     6   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "2     RES  ARG  G     7   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "3     RES  PRO  G     8   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "4     RES  LEU  G     9   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "...   ...  ... ..   ...    ...   ...    ...   ...    ...   ...    ...   ...   \n",
      "2028  RES  GLU  G  2046   0.00   0.0   0.00   0.0   0.00   0.0   0.00   0.0   \n",
      "2029  RES  LYS  G  2047  41.70  20.8  34.14  20.9   7.56  20.1  33.33  28.5   \n",
      "2030  RES  TYR  G  2048  69.31  32.5  47.27  26.6  22.04  62.3  45.66  33.5   \n",
      "2031  RES  GLU  G  2049   0.14   0.1   0.14   0.1   0.00   0.0   0.14   0.2   \n",
      "2032  RES  GLN  G  2050  66.42  37.2  63.81  45.2   2.62   6.9  43.78  83.8   \n",
      "\n",
      "         12    13  \n",
      "0      0.00   0.0  \n",
      "1      0.00   0.0  \n",
      "2      0.00   0.0  \n",
      "3      0.00   0.0  \n",
      "4      0.00   0.0  \n",
      "...     ...   ...  \n",
      "2028   0.00   0.0  \n",
      "2029   8.37   9.9  \n",
      "2030  23.67  31.1  \n",
      "2031   0.00   0.0  \n",
      "2032  22.64  17.9  \n",
      "\n",
      "[2033 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2.4 Perform the subtraction of ABS and REL values by the individual DataFrame to the combined DataFrame(each chain part)\n",
    "#     write the results to file\n",
    "def subtract_rsa_values_and_write_to_file(df_chain1, df_chain2, df_combined, df_combined_name):\n",
    "    # Play with names\n",
    "    basename = df_combined_name.split('.')[0]\n",
    "    pdb_id, chain_set = basename.split('_')\n",
    "    chain1_id, chain2_id = chain_set[0], chain_set[1]\n",
    "    \n",
    "    # Copy DataFrames to avoid altering original data and convert all values to float\n",
    "    df1 = df_chain1.copy()\n",
    "    df2 = df_chain2.copy()\n",
    "    df_combined_1 = df_combined[df_combined.iloc[:, 2] == chain1_id].copy().reset_index(drop=True)  # Filter for chain1\n",
    "    df_combined_2 = df_combined[df_combined.iloc[:, 2] == chain2_id].copy().reset_index(drop=True)  # Filter for chain2\n",
    "    \n",
    "    # print(f'The df_combined_1 is prepared, shape: {df_combined_1.shape} \\n{df_combined_1}')\n",
    "    # print(f'The df_combined_2 is prepared, shape: {df_combined_2.shape} \\n{df_combined_2}')\n",
    "    # # Define file names for the output\n",
    "    # df_combined_1_filename = f'{pdb_id}_{chain1_id}_from_combined.csv'\n",
    "    # df_combined_2_filename = f'{pdb_id}_{chain2_id}_from_combined.csv'\n",
    "    # # Write df_combined_1 to a CSV file\n",
    "    # df_combined_1.to_csv(df_combined_1_filename, index=False)\n",
    "    # print(f'df_combined_1 written to file: {df_combined_1_filename}')\n",
    "    # # Write df_combined_2 to a CSV file\n",
    "    # df_combined_2.to_csv(df_combined_2_filename, index=False)\n",
    "    # print(f'df_combined_2 written to file: {df_combined_2_filename}')\n",
    "    \n",
    "    # Ensure subtraction is only applied to value columns, i.e. excluding identifier columns (the first 4 columns)\n",
    "    data1 = df1.columns[4:]\n",
    "    data2 = df2.columns[4:]\n",
    "    data_combined_1 = df_combined_1.columns[4:]\n",
    "    data_combined_2 = df_combined_2.columns[4:]\n",
    "    # Convert all data to numeric for subtraction\n",
    "    for df in [df1, df2, df_combined_1, df_combined_2]:\n",
    "        for col in df.columns[4:]:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Perform subtraction\n",
    "    df_subtracted_1 = df1[data1].subtract(df_combined_1[data_combined_1], fill_value=0)\n",
    "    df_subtracted_2 = df2[data2].subtract(df_combined_2[data_combined_2], fill_value=0)\n",
    "    # print(f'\\nSubtraction completed by {pdb_id}_{chain1_id} - {basename}({chain1_id} part)')\n",
    "    # print(f'{df_subtracted_1}')\n",
    "    # print(f'Subtraction completed by {pdb_id}_{chain2_id} - {basename}({chain2_id} part)')\n",
    "    # print(f'{df_subtracted_2}')\n",
    "    \n",
    "    # Extract identifier columns from df1 and df2\n",
    "    identifiers_1 = df1.iloc[:, :4].reset_index(drop=True)\n",
    "    identifiers_2 = df2.iloc[:, :4].reset_index(drop=True)\n",
    "    # Concatenate identifier columns with subtracted DataFrames\n",
    "    df_subtracted_1 = pd.concat([identifiers_1, df_subtracted_1.reset_index(drop=True)], axis=1)\n",
    "    df_subtracted_2 = pd.concat([identifiers_2, df_subtracted_2.reset_index(drop=True)], axis=1)\n",
    "    # Consist the data decimal places (ABS:2, REL:1)\n",
    "    df_subtracted_1 = format_decimal_places(df_subtracted_1)\n",
    "    df_subtracted_2 = format_decimal_places(df_subtracted_2)\n",
    "    # Reorder columns into numerical sequence by the NUM column\n",
    "    df_subtracted_1 = df_subtracted_1.sort_index()\n",
    "    df_subtracted_2 = df_subtracted_2.sort_index()\n",
    "    \n",
    "    # Write to .csv file\n",
    "    df_subtracted_1_filename = f'{pdb_id}_{chain1_id}-{chain_set}.csv'\n",
    "    df_subtracted_2_filename = f'{pdb_id}_{chain2_id}-{chain_set}.csv'\n",
    "    # sentence = 'REM,File of subtracted absolute (ABS) and relative (REL) (%) accessibilities for\\n'\n",
    "    header_1 = 'REM,RES_NUM, , ,All-atoms, ,Total-Side, ,Main-Chain, ,Non-polar, ,All-polar\\n'\n",
    "    header_2 = 'REM,AA,CHAIN,NUM,ABS,REL,ABS,REL,ABS,REL,ABS,REL,ABS,REL\\n'\n",
    "    with open(df_subtracted_1_filename, 'w') as file:\n",
    "        # file.write(sentence)\n",
    "        file.write(header_1)\n",
    "        file.write(header_2)\n",
    "        df_subtracted_1.to_csv(file, sep=',', index=False, header=False, mode='a')  # Append mode in case we want to add more lines\n",
    "        print(f'\\nWritten subtraction file {df_subtracted_1_filename}, aa: {len(df_subtracted_1)}')\n",
    "    with open(df_subtracted_2_filename, 'w') as file:\n",
    "        # file.write(sentence)\n",
    "        file.write(header_1)\n",
    "        file.write(header_2)\n",
    "        df_subtracted_2.to_csv(file, sep=',', index=False, header=False, mode='a')  # Append mode in case we want to add more lines\n",
    "        print(f'Written subtraction file {df_subtracted_2_filename}, aa: {len(df_subtracted_2)}')\n",
    "    \n",
    "    return df_subtracted_1, df_subtracted_2, df_subtracted_1_filename, df_subtracted_2_filename\n",
    "\n",
    "df_subtracted_1, df_subtracted_2, df_subtracted_1_filename, df_subtracted_2_filename = subtract_rsa_values_and_write_to_file(df_chain1, df_chain2, df_combined, df_combined_name)\n",
    "print(f'{df_subtracted_1_filename}: \\n{df_subtracted_1}')\n",
    "print(f'{df_subtracted_2_filename}: \\n{df_subtracted_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written buried residues to file, PDB model 2uv8 complex AG, by chain A, aa: 109\n",
      "Written buried residues to file, PDB model 2uv8 complex AG, by chain G, aa: 157\n"
     ]
    }
   ],
   "source": [
    "# 2.5 Check for buried residues and write to files\n",
    "def check_for_buried_residues_and_write_to_file(df_subtracted_1, df_subtracted_2, df_subtracted_1_filename, df_subtracted_2_filename):\n",
    "    # Play with names\n",
    "    basename1 = df_subtracted_1_filename.split('.')[0]\n",
    "    basename2 = df_subtracted_2_filename.split('.')[0]\n",
    "    pdb_id, subtracted_name1 = basename1.split('_')\n",
    "    pdb_id, subtracted_name2 = basename2.split('_')\n",
    "    chain1_id, combined_id = subtracted_name1.split('-')\n",
    "    chain2_id, combined_id = subtracted_name2.split('-')\n",
    "    # Convert the relevant columns to numeric for comparison in the next step\n",
    "    df_subtracted_1.iloc[:, 4] = pd.to_numeric(df_subtracted_1.iloc[:, 4], errors='coerce')\n",
    "    df_subtracted_1.iloc[:, 5] = pd.to_numeric(df_subtracted_1.iloc[:, 5], errors='coerce')\n",
    "    df_subtracted_2.iloc[:, 4] = pd.to_numeric(df_subtracted_2.iloc[:, 4], errors='coerce')\n",
    "    df_subtracted_2.iloc[:, 5] = pd.to_numeric(df_subtracted_2.iloc[:, 5], errors='coerce')\n",
    "    # Check: counted as a buried residue if the All-atoms-ABS or All-atoms-REL is greater than 0\n",
    "    buried_residues_in_chain1 = df_subtracted_1.loc[(df_subtracted_1.iloc[:, 4] > 0) | (df_subtracted_1.iloc[:, 5] > 0)]\n",
    "    buried_residues_in_chain2 = df_subtracted_2.loc[(df_subtracted_2.iloc[:, 4] > 0) | (df_subtracted_2.iloc[:, 5] > 0)]\n",
    "    \n",
    "    # Write to .csv files\n",
    "    filename1 = f'{pdb_id}_{chain1_id}-{combined_id}_buried_residues.csv'\n",
    "    filename2 = f'{pdb_id}_{chain2_id}-{combined_id}_buried_residues.csv'\n",
    "    # sentence = 'REM,File of subtracted absolute (ABS) and relative (REL) (%) accessibilities for\\n'\n",
    "    header_1 = 'REM,RES_NUM, , ,All-atoms, ,Total-Side, ,Main-Chain, ,Non-polar, ,All-polar\\n'\n",
    "    header_2 = 'REM,AA,CHAIN,NUM,ABS,REL,ABS,REL,ABS,REL,ABS,REL,ABS,REL\\n'\n",
    "    if not buried_residues_in_chain1.empty:\n",
    "        with open(filename1, 'w') as file:\n",
    "            # file.write(sentence)\n",
    "            file.write(header_1)\n",
    "            file.write(header_2)\n",
    "            buried_residues_in_chain1.to_csv(file, sep=',', index=False, header=False, mode='a')\n",
    "        print(f'Written buried residues to file, PDB model {pdb_id} complex {combined_id}, by chain {chain1_id}, aa: {len(buried_residues_in_chain1)}')\n",
    "        # print(f'{buried_residues_in_chain1}')\n",
    "    else:\n",
    "        print(f'No residues buried in PDB model {pdb_id} complex {combined_id}, by chain {chain1_id}')\n",
    "    if not buried_residues_in_chain2.empty:\n",
    "        with open(filename2, 'w') as file:\n",
    "            # file.write(sentence)\n",
    "            file.write(header_1)\n",
    "            file.write(header_2)\n",
    "            buried_residues_in_chain2.to_csv(file, sep=',', index=False, header=False, mode='a')\n",
    "        print(f'Written buried residues to file, PDB model {pdb_id} complex {combined_id}, by chain {chain2_id}, aa: {len(buried_residues_in_chain2)}')\n",
    "        # print(f'{buried_residues_in_chain2}')\n",
    "    else:\n",
    "        print(f'No residues buried in PDB model {pdb_id} complex {combined_id}, by chain {chain2_id}')\n",
    "\n",
    "       \n",
    "check_for_buried_residues_and_write_to_file(df_subtracted_1, df_subtracted_2, df_subtracted_1_filename, df_subtracted_2_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
